{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4adbc7e4",
   "metadata": {},
   "source": [
    "# 1. Vocabulary initizlization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a54ac15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab: dict[int, bytes] = {i: bytes([i]) for i in range(256)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f50f800a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd', '你好')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"你好\".encode(\"utf-8\"), b'\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd'.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8ad1e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'\\x00', b'\\x01', b'B')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0], vocab[1], vocab[66]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d4e63",
   "metadata": {},
   "source": [
    "# 2. Pre-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51ffd7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cb4dbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some (0, 4)\n",
      " text (4, 9)\n",
      " that (9, 14)\n",
      " i (14, 16)\n",
      "'ll (16, 19)\n",
      " pre (19, 23)\n",
      "- (23, 24)\n",
      "tokenize (24, 32)\n"
     ]
    }
   ],
   "source": [
    "char_iter = re.finditer(PAT, \"some text that i'll pre-tokenize\")\n",
    "for c in char_iter:\n",
    "    print(c.group(), c.span())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60b53acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\\n<|endoftext|>\\nOnce upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\\nTom asked his friend, Sam, to help him search for the ball. They looked high a'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/home/lin/AI-Learning/CS336/data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "def get_data(path: str) -> list[str]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "data = get_data(data_path)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f52e37ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "test_pre_tokenized = defaultdict(int)\n",
    "for c in re.findall(PAT, \"some text that i'll pre-tokenize some some\"):\n",
    "    test_pre_tokenized[c.encode()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a54b4d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {b'some': 1,\n",
       "             b' text': 1,\n",
       "             b' that': 1,\n",
       "             b' i': 1,\n",
       "             b\"'ll\": 1,\n",
       "             b' pre': 1,\n",
       "             b'-': 1,\n",
       "             b'tokenize': 1,\n",
       "             b' some': 2})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pre_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5072a309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('s', 'o', 'm', 'e'): 1,\n",
       "             (' ', 't', 'e', 'x', 't'): 1,\n",
       "             (' ', 't', 'h', 'a', 't'): 1,\n",
       "             (' ', 'i'): 1,\n",
       "             (\"'\", 'l', 'l'): 1,\n",
       "             (' ', 'p', 'r', 'e'): 1,\n",
       "             ('-',): 1,\n",
       "             ('t', 'o', 'k', 'e', 'n', 'i', 'z', 'e'): 1,\n",
       "             (' ', 's', 'o', 'm', 'e'): 2})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pre_tokenized = defaultdict(int)\n",
    "for c in re.finditer(PAT, \"some text that i'll pre-tokenize some some\"):\n",
    "    # test_pre_tokenized[c.group().encode()] += 1\n",
    "    key = tuple([c for c in iter(c.group())]) # TODO: 这里测试先不使用encode\n",
    "    test_pre_tokenized[key] += 1\n",
    "\n",
    "test_pre_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16684aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting pairs...: 100%|███████████████████| 9/9 [00:00<00:00, 114044.52words/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "test_pairs_counter = Counter()\n",
    "max_value = 0\n",
    "max_pair = None   # 获取出现最多次(在有多个相同的情况下, 取最后一个)的pair\n",
    "\n",
    "# 更新Counter, 同时返回最大的pair以及出现的最多的次数\n",
    "def update_pairs_counter(counter, word, num):\n",
    "    for c1, c2 in zip(pair[:-1], pair[1:]):\n",
    "        counter[(c1, c2)] += v\n",
    "\n",
    "\n",
    "for k, v in tqdm(test_pre_tokenized.items(), desc=\"Counting pairs...\", unit='words', ncols=80):\n",
    "    for c1, c2 in zip(k[:-1], k[1:]):\n",
    "        test_pairs_counter[(c1, c2)] += v\n",
    "        if test_pairs_counter[(c1, c2)] >= max_value:\n",
    "            max_value = test_pairs_counter[(c1, c2)]\n",
    "            max_pair = (c1, c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0821761a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('s', 'o'): 3,\n",
       "         ('o', 'm'): 3,\n",
       "         ('m', 'e'): 3,\n",
       "         (' ', 't'): 2,\n",
       "         (' ', 's'): 2,\n",
       "         ('t', 'e'): 1,\n",
       "         ('e', 'x'): 1,\n",
       "         ('x', 't'): 1,\n",
       "         ('t', 'h'): 1,\n",
       "         ('h', 'a'): 1,\n",
       "         ('a', 't'): 1,\n",
       "         (' ', 'i'): 1,\n",
       "         (\"'\", 'l'): 1,\n",
       "         ('l', 'l'): 1,\n",
       "         (' ', 'p'): 1,\n",
       "         ('p', 'r'): 1,\n",
       "         ('r', 'e'): 1,\n",
       "         ('t', 'o'): 1,\n",
       "         ('o', 'k'): 1,\n",
       "         ('k', 'e'): 1,\n",
       "         ('e', 'n'): 1,\n",
       "         ('n', 'i'): 1,\n",
       "         ('i', 'z'): 1,\n",
       "         ('z', 'e'): 1})"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pairs_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "353ff1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, ('m', 'e'))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_value, max_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ('s', 'o', 'm', 'e')\n",
    "max_pair = ('s', 'e')\n",
    "start_index = -1\n",
    "\n",
    "def get_match_pair_index(target: tuple[str], max_pair: tuple[str]) -> int:\n",
    "    if max_pair[0] not in pair or max_pair[1] not in pair:\n",
    "        return -1\n",
    "    for i in range(len(target) - 1):\n",
    "        if max_pair == (target[i], target[i + 1]):\n",
    "            return i\n",
    "    return -1 \n",
    "\n",
    "def update_tokenization(tokenized_dict, max_pair):\n",
    "    for pair in tokenized_dict:\n",
    "        start = get_match_pair_index(pair, max_pair)\n",
    "        if start != -1:\n",
    "            # 先更新pair, 然后根据新的pair, 更新Counter\n",
    "            new_pair = list(pair)\n",
    "            new_pair[start] = new_pair[start] + new_pair[start + 1]\n",
    "            new_pair.pop(start + 1)\n",
    "            tokenized_dict[tuple(new_pair)] += tokenized_dict[pair]\n",
    "            del tokenized_dict[pair]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36226ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 2, 0: 1, 1: 1, 2: 1, 3: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(list(range(10)) + [4])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27454071",
   "metadata": {},
   "outputs": [],
   "source": [
    "del c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34bb8cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92222849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'ab'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = (b'a', b'b')\n",
    "b\"\".join(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0a30d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Dataset
Dataset:
  train_path: None
  val_path: None


# Model config
Model:
  vocab_size: 10000
  d_model: 256
  d_ff: 1024
  num_layers: 20
  num_heads: 8
  rope_theta: 10000
  max_seq_len: 10000


# Optimizer config
Optimizer:
  lr: 1e-4
  betas: (0.9, 0.999)
  weight_decay: 1e-5
  eps: 1e-8


# Scheduler
Scheduler:
  max_learning_rate: 1e-4
  min_learning_rate: 1e-5
  warmup_iters: 1000
  consine_cycle_iters: 5000


# training
Train:
  steps: 20000
  batch_size: 256
  context_length: 64
  device: cuda:0
  grad_clip:
    max_l2_norm: 100
    eps: 1e-6



# valid, checkpoint
Test:
  save_checkpoint_period: 1000
  valid_period: 1000
  checkpoint: None
  save_checkpoint_folder: checkpoint
  resume: False
  resume_checkpoint_path: None
  
# wandb
WandB:
  # project: CS336_Assignment1_Train_LLM
  project: CS336_Assignment1_Train_LLM
  name: Batch
# Dataset
Dataset:
  train_path: data\TinyStoriesV2-GPT4-train.dat
  val_path: data\TinyStoriesV2-GPT4-valid.dat


# Model config
Model:
  vocab_size: 10000
  d_model: 512
  d_ff: 1344
  num_layers: 4
  num_heads: 16
  rope_theta: 10000
  max_seq_len: 256


# Optimizer config
Optimizer:
  lr: 1e-4
  betas: [0.9, 0.999]
  weight_decay: 1e-5
  eps: 1e-8


# Scheduler
Scheduler:
  max_learning_rate: 1e-4
  min_learning_rate: 1e-5
  warmup_iters: 1000
  consine_cycle_iters: 20000


# training
Train:
  steps: 20000
  batch_size: 64
  context_length: 256
  device: cuda:0
  grad_clip:
    max_l2_norm: 1.0
    eps: 1e-6



# valid, checkpoint
Test:
  save_checkpoint_period: 1000
  valid_period: 1000
  checkpoint: None
  save_checkpoint_folder: checkpoint
  resume: False
  resume_checkpoint_path: None
  
# wandb
WandB:
  project: CS336_Assignment1_Train_LLM
  name: Batch And Steps Set
  notes: 先让Batch最大化(根据GPU显存)
